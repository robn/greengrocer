#!/usr/bin/env perl

package greengrocer;

use 5.014;
use warnings;
use strict;

use Lucy;
use Path::Tiny;
use AnyEvent::Log;
use AnyEvent::Handle;
use AnyEvent::Handle::UDP;
use Date::Format qw(time2str);
use Date::Parse qw(str2time);
use Getopt::Std qw(getopts);
use Time::HiRes qw(gettimeofday tv_interval);
use JSON::XS qw(decode_json);
use IO::Socket::INET;
use Socket qw(SOL_SOCKET SO_RCVBUF);
use POSIX qw(_exit);

getopts('d:h', \my %OPTS);

my ($INDEX_DIR, @EXTRA_INDEX_DIRS) = split ':', $OPTS{d};

my $ACTION = shift @ARGV;
my $handler = {
    agent    => \&agent,
    search   => \&search,
    rollup   => \&rollup,
    optimize => \&optimize,
    web      => \&web,
}->{$ACTION // ''};

say STDERR "E: unknown action: $ACTION" if $ACTION && !$handler;
usage() if !$INDEX_DIR || $OPTS{h} || !$ACTION || !$handler;

path($INDEX_DIR)->mkpath;

$handler->(@ARGV);

sub agent {
    local @ARGV = @_;
    getopts('l:p:i:h:', \my %opts);

    my $ip = $opts{l} // '0.0.0.0';
    my $port = $opts{p} // 5514;

    AnyEvent::Log::ctx->log_to_warn;
    AnyEvent::Log::ctx->fmt_cb(sub { format_log($_[3]) });
    my $log = AnyEvent::Log::logger("notice");

    my $cv = AnyEvent::condvar;

    my $interval = (0+($opts{i} // 0)) || 10;

    my $roll_hours = $opts{h} // 24;

    $log->("commit interval is $interval, rolling every $roll_hours hours");

    $roll_hours %= 24;

    my @queue;

    my $child_guard;
    my $queue_guard; $queue_guard = AnyEvent->timer(
        after    => $interval,
        interval => $interval,
        cb => sub {
            return unless @queue;

            if ($child_guard) {
                $log->("commit in progress, deferring");
                return;
            }

            my @commit_queue = @queue;
            undef @queue;

            my $pid = fork;
            if ($pid < 0) {
                $log->("fork failed: $!");
                $log->("returning ", scalar(@commit_queue), " lines to the queue");
                push @queue, @commit_queue;
                return;
            }

            if ($pid) {
                $child_guard = AnyEvent->child(
                    pid => $pid,
                    cb => sub {
                        undef $child_guard;
                    },
                );
                return;
            }

            my %indexers;

            my ($t_start, $t_add, $t_commit);

            $t_start = [gettimeofday];

            for my $data (@commit_queue) {
                @$data{qw(program pid)} = delete($data->{syslogtag}) =~ m/^([^\[]+)(?:\[(\d+)\])?:$/;
                $data->{pid} //= '';

                my ($y, $m, $d, $h) = $data->{timestamp} =~ m/^(\d+)-(\d+)-(\d+)T(\d+)/;
                unless (defined $y && defined $m && defined $d && defined $h) {
                    $log->("E: malformed timestamp: $data->{timestamp}");
                }

                my $key = "$y$m$d";
                if ($roll_hours) {
                    my $roll = int($h / $roll_hours);
                    $key .= $roll >= 10 ? "_$roll" : "_0$roll";
                }

                ($indexers{$key} ||= Lucy::Index::Indexer->new(
                    index    => path($INDEX_DIR, $key),
                    schema => schema(),
                    create => 1,
                ))->add_doc($data);
            }

            $t_add = [gettimeofday];

            for my $indexer (values %indexers) {
                $indexer->commit;
            };

            $t_commit = [gettimeofday];

            my $e_add     = tv_interval($t_start, $t_add);
            my $e_commit  = tv_interval($t_add, $t_commit);

            my $n_items = scalar @commit_queue;

            my $r_add    = $e_add / $n_items;
            my $r_commit = $e_commit / $n_items;

            $log->("indexed %d lines [add %.3f (%.6f) commit %.3f (%.6f)]", $n_items, $e_add, $r_add, $e_commit, $r_commit);

            POSIX::_exit(0);
        },
    );

    my $server; $server = AnyEvent::Handle::UDP->new(
        bind => [$ip, $port],
        on_recv => sub {
            my ($data, $handle, $client_addr) = @_;
            eval { push @queue, decode_json($data) };
            if (my $err = $@) {
                $log->("data error: $@: $data");
            }
        },
        on_error => sub {
            my ($handle, $is_fatal, $msg) = @_;
            my $peer = $handle->getpeername;
            $log->("$peer error: $msg");
            $handle->destroy;
        },
    );

    $server->fh->setsockopt(SOL_SOCKET, SO_RCVBUF, 64*1024*1024) or
      $log->("setsockopt: $!");

    my $rcvbuf = $server->fh->getsockopt(SOL_SOCKET, SO_RCVBUF);
    $log->("listening on $ip:$port, receive buffer is $rcvbuf bytes");

    $cv->recv;
}

sub search {
    local @ARGV = @_;
    getopts('s:e:j', \my %opts);

    run_search(
        start     => $opts{s},
        end       => $opts{e},
        query     => join(' ', @ARGV),
        collector => sub {
            my ($searcher, $query) = @_;
            greengrocer::CLICollector->new(searcher => $searcher, json => !!$opts{j}),
        },
        error => sub {
            my ($msg) = @_;
            say STDERR "E: $msg";
            usage();
        },
    );
}

sub rollup {
    local @ARGV = @_;
    getopts('fk', \my %opts);

    my ($date, $target_dir) = @ARGV;
    unless ($date && $target_dir) {
        usage();
    }

    my $today = time2str("%Y%m%d", time);
    if (!$opts{f} && substr($date, 0, 8) == $today) {
        warn <<EOF;
E: This is today's date. Rolling up today's indexes could cause confusion later
because if new logs arrive you'll end up with multiple indexes with the same
name, which isn't recommended. If you really want to do this, rerun rollup with
the -f switch.
EOF
        exit 1;
    }

    my $target_index_dir = path($target_dir, $date);
    if ($target_index_dir->is_dir) {
        warn "E: Target index already exists, and I won't overwrite it. Move it out of the way first.\n";
        exit 1;
    }

    my @indexes =
        grep {
            my $basename = $_->basename;
            substr($basename, 0, 8) eq $date || $basename eq $date;
        } map {
            path($_)->absolute->children
        } ($INDEX_DIR, @EXTRA_INDEX_DIRS);

    my $temp_dir = path($target_dir, "tmp");
    $temp_dir->mkpath;

    my $temp_index_dir = path($temp_dir, $date);
    $temp_index_dir->remove_tree;

    my $indexer = eval {
        Lucy::Index::Indexer->new(
            index  => $temp_index_dir,
            schema => schema(),
            create => 1,
        );
    };
    if ($@) {
        warn "E: couldn't open new index in $temp_index_dir: $@\n";
        exit 1;
    }

    for my $index (@indexes) {
        warn "rolling up $index into $temp_index_dir...\n";
        $indexer->add_index($index);
    }

    warn "optimizing new index...\n";
    $indexer->optimize;

    warn "committing new index...\n";
    $indexer->commit;

    warn "moving new index into $target_index_dir...\n";
    $temp_index_dir->move($target_index_dir);

    unless ($opts{k}) {
        for my $index (@indexes) {
            warn "deleting original index $index...\n";
            $index->remove_tree;
        }
    }

    warn "done\n";
}

sub optimize {
    local @ARGV = @_;

    for my $index (@ARGV) {
        my $indexer = eval {
            Lucy::Index::Indexer->new(
                index  => path($INDEX_DIR, $index),
                schema => schema(),
            );
        };
        if ($@) {
            warn "E: couldn't open index $index, skipping\n";
            next;
        }

        print STDERR "indexing $index... ";
        STDERR->flush;

        $indexer->optimize;
        $indexer->commit;

        say STDERR "done";
    }
}

sub web {
    my @modules = qw(Starman Plack Plack::Middleware::Redirect Atto);
    my @missing = grep { ! eval "require $_" } (@modules, 'Starman::Server', 'Plack::Builder');
    if (@missing) {
        say STDERR "E: the web server requires these additional modules: @modules";
        say STDERR "E: try running: cpanm @modules";
        exit 1;
    }

    local @ARGV = @_;
    getopts('l:p:', \my %opts);

    my $ip = $opts{l} // '0.0.0.0';
    my $port = $opts{p} // 5515;

    my $server = Starman::Server->new;

    my $log = sub { $server->log(2, format_log(shift)) };

    my $builder = Plack::Builder->new;
    $builder->add_middleware('Redirect', url_patterns => [ '^/$' => ['/ui/', 302] ]);
    $builder->add_middleware('Static', path => sub { s{^/ui/$}{/ui/index.html}; m{^/ui/} }, root => './');
    $builder->mount('/', greengrocer::SearchServer->psgi(logger => $log));

    $server->run($builder->to_app, { host => $ip, port => $port });
}

sub format_log {
    sprintf "[greengrocer] %s %s\n", time2str("%Y-%m-%dT%H:%M:%S", time), shift;
}

sub schema {
    # {"pid":"4090299","timestamp":"2015-12-21T18:47:30.697022-05:00","program":"sloti30t15/calalarmd","host":"imap30","message":" processing alarms"}

    state $schema = do {
        my $s = Lucy::Plan::Schema->new;

        $s->spec_field(
            name => 'timestamp',
            type => Lucy::Plan::StringType->new(
                sortable => 1,
            ),
        );
        $s->spec_field(
            name => 'host',
            type => Lucy::Plan::StringType->new,
        );
        $s->spec_field(
            name => 'program',
            type => Lucy::Plan::FullTextType->new(
                analyzer => Lucy::Analysis::StandardTokenizer->new,
            ),
        );
        $s->spec_field(
            name => 'pid',
            type => Lucy::Plan::StringType->new,
        );
        $s->spec_field(
            name => 'message',
            type => Lucy::Plan::FullTextType->new(
                analyzer => Lucy::Analysis::StandardTokenizer->new,
            ),
        );

        $s;
    };

    return $schema;
}

sub run_search {
    my (%args) = @_;

    my $now = time;
    my $start_date = time2str("%Y%m%d", str2time($args{start}) // $now);
    my $end_date   = time2str("%Y%m%d", str2time($args{end}) // ($now+86400));
    if ($start_date ge $end_date) {
        $args{error}->("invalid date range $start_date - $end_date");
        return;
    }

    my $query_parser = Lucy::Search::QueryParser->new(schema => schema());
    $query_parser->set_heed_colons(1);
    my $query = $query_parser->parse($args{query} // '');

    my @indexes =
        grep {
            my ($basename) = $_->basename =~ m/^(\d+)/;
            $basename //= -1;
            $basename >= $start_date && $basename < $end_date;
        } map {
            path($_)->absolute->children
        } ($INDEX_DIR, @EXTRA_INDEX_DIRS);

    my @searchers = map { Lucy::Search::IndexSearcher->new(index => $_) } @indexes;
    my $searcher = Lucy::Search::PolySearcher->new(
        schema => schema(),
        searchers => \@searchers,
    );

    my $collector = $args{collector}->($searcher, $query);

    do {
        no warnings 'uninitialized'; # working around slight bug inside Lucy
        $searcher->collect(
            query => $query,
            collector => $collector,
        );
    };
}

sub usage {
    my $action = $ACTION // '';

    if ($action eq 'agent') {
        print STDERR <<USAGE;
Usage: greengrocer -d <index-dir> agent [opts...]

Starts the log collection agent. The agent itself will log info about
its activities to standard error.

Options:
    -l <ip>       - IP address to listen on [default: all]
    -p <port>     - port to listen on [default: 5514]
    -i <interval> - commit logs to index every N seconds [default: 10]
    -h <hours>    - create new index within each day every N hours [default: 24]
USAGE
    }
    elsif ($action eq 'search') {
        print STDERR <<USAGE;
Usage: greengrocer -d <index-dir> search [opts...] <query...>

Searches for log lines matching the given query.

You can specify multiple index dirs with -d /dir1:/dir2.

Options:
    -s <date> - start date to search (inclusive) [default: today]
    -e <date> - end date to search (exclusive) [default: tomorrow]
    -j        - JSON output
USAGE
    }
    elsif ($action eq 'web') {
        print STDERR <<USAGE;
Usage: greengrocer -d <index-dir> web [opts...]

Starts the web search API server. The server will log requests to standard
error.

You can specify multiple index dirs with -d /dir1:/dir2.

Options:
    -l <ip>       - IP address to listen on [default: all]
    -p <port>     - port to listen on [default: 5515]

To run a search, make a web request to the /search endpoint with the following
query parameters:

    start - start date to search (inclusive) [default: today]
    end   - end date to search (exclusive) [default: tomorrow]
    query - query string

Example:

    \$ curl http://127.0.0.1:5515/search?query=info

USAGE
    }
    elsif ($action eq 'rollup') {
        print STDERR <<USAGE;
Usage: greengrocer -d <index-dir> rollup [opts...] <index> <target-index-dir>

Move/merge indexes. Copies the given index to the target dir and removes the
original. If there are multiple indexes matching the named index (ie when using
hourly rolls), they will all be moved into a single index.

Options:
    -f - force move of an active index (ie today's)
    -k - keep; don't remove the original indexes. Useful for testing
USAGE
    }
    elsif ($action eq 'optimize') {
        print STDERR <<USAGE;
Usage: greengrocer -d <index-dir> optimize <indexes...>

Optimizes a indexes for fast searching. Indexes can become fragmented after a
lot of writes, which can make searching slower. Optimizing reprocesses the log
lines into a new unfragmented index which takes less effort to search. It
usually won't make the index significantly smaller.

Optimizing an index can take a long time (minutes) and locks the index for
writing, so new log entries cannot be added. For this reason don't try to
optimize an active index (that is, today's) or you'll likely end up losing
incoming log lines.
USAGE
    }
    else {
        print STDERR <<USAGE;
Usage: greengrocer -d <index-dir> <action> [opts...]

Global options:

    -d <index-dir> - location to store indexes. Multiple locations can be
                     specified by separating them with a colon. Most actions
                     will only use the first one; see help on specific actions
                     for more details.

Actions:
    agent    - run the log collection agent
    search   - show log lines matching a query
    web      - run the web search API server
    rollup   - move/merge indexes
    optimize - optimize indexes for searching

For more help on action: greengrocer <action> -h
USAGE
    }

    exit 1;
}


package greengrocer::CLICollector;

use parent qw(Lucy::Search::Collector);
use JSON::XS;

our %self;

sub new {
    my ($class, %args) = @_;

    my $ref = $class->SUPER::new;

    $self{$ref} = {
        searcher => $args{searcher},
        base     => 0,
        json     => $args{json},
    };

    return $ref;
}

sub DESTROY {
    my ($ref) = @_;
    my $self = delete $self{$ref};
    if ($self->{json}) {
        if ($self->{started}) {
            say "\n]";
        }
        else {
            say "[]";
        }
    }
}

sub _output_text {
    my ($self, $doc) = @_;
    my ($timestamp, $host, $program, $pid, $message) = map { $doc->{$_} } qw(timestamp host program pid message);
    $pid = "[$pid]" if $pid;
    printf "%s %s %s%s:%s\n", $timestamp, $host, $program, $pid, $message;
}

sub _output_json {
    my ($self, $doc) = @_;

    unless ($self->{started}) {
        say '[';
        $self->{started} = 1;
    }
    else {
        say ',';
    }

    print '  ', encode_json({ map { $_ => $doc->{$_} } qw(timestamp host program pid message) });
}

sub collect {
    my ($ref, $doc_id) = @_;
    my $self = $self{$ref};
    my $doc = $self->{searcher}->fetch_doc($self->{base} + $doc_id);
    if ($self->{json}) {
        _output_json($self, $doc);
    }
    else {
        _output_text($self, $doc);
    }
}

sub set_base {
    my ($ref, $base) = @_;
    my $self = $self{$ref};
    $self->{base} = $base;
}

sub need_score { 0 }


package greengrocer::ServerCollector;

use parent qw(Lucy::Search::Collector);

our %self;

sub new {
    my ($class, %args) = @_;

    my $ref = $class->SUPER::new;

    $self{$ref} = {
        searcher => $args{searcher},
        base     => 0,
        matches  => [],
    };

    return $ref;
}

sub matches {
    my ($ref) = @_;
    my $self = $self{$ref};
    $self->{matches};
}

sub DESTROY {
    my ($ref) = @_;
    my $self = delete $self{$ref};
}

sub collect {
    my ($ref, $doc_id) = @_;
    my $self = $self{$ref};
    my $doc = $self->{searcher}->fetch_doc($self->{base} + $doc_id);
    push @{$self->{matches}}, { map { $_ => $doc->{$_} } qw(timestamp host program pid message) };
}

sub set_base {
    my ($ref, $base) = @_;
    my $self = $self{$ref};
    $self->{base} = $base;
}

sub need_score { 0 }


package greengrocer::SearchServer;

our $log;

sub psgi {
    my ($class, %args) = @_;
    $log = $args{logger};

    Atto->import(qw(search));
    Atto->psgi;
}

sub search {
    my (%args) = @_;

    my $info = join(' ', map { "$_=$args{$_}" } sort keys %args);

    my $collector;

    greengrocer::run_search(
        start     => $args{start},
        end       => $args{end},
        query     => $args{query},
        collector => sub {
            my ($searcher, $query) = @_;
            $collector = greengrocer::ServerCollector->new(searcher => $searcher);
            return $collector;
        },
        error => sub {
            my ($msg) = @_;
            $log->("search: $info [FAILED: $msg]");
            die "E: $msg\n";
        },
    );

    my $matches = $collector->matches;
    my $count = @$matches;

    $log->(sprintf "search: %s [matches %d]", join(' ', map { "$_=$args{$_}" } sort keys %args), scalar @$matches);

    return {
        matches => $matches,
        count   => $count,
    };
}
